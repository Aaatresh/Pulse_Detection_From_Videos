{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulse Detection from Color Changes\n",
    "\n",
    "Eulerian Video Magnification is based on : https://github.com/flyingzhao/PyEVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all essential libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import scipy.fftpack as fftpack\n",
    "from scipy import signal, stats\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Head/ROI Dectection\n",
    "\n",
    "1) **haarCascadeFaceDet** :: Takes in an image and outputs the x,y co-ordinates of the upper left corner of the phase with the width and height as well<br/><br/>\n",
    "2) **getFace** :: Takes in an image and a tuple with containg the (x,y,w,h) properties of a bounding box and outputs the facial reigon. The eyes are blacked out here<br/><br/>\n",
    "3) **findMaxFace** :: Takes in a list of faces detected by the harcascade and outputs the (x,y,w,h) tuple corresponding to the face with the maximum area<br/><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haarCascadeFaceDet(image):\n",
    "    # Define the Haar Cascade\n",
    "    face_cascade = cv2.CascadeClassifier('./resources/haarcascade_frontalface_default.xml')\n",
    "    # Use the cascade to extract the face\n",
    "    faces = face_cascade.detectMultiScale(image, 1.3, 5)\n",
    "    # Reain the regoin with the maxmimum face i.e facial reigon with the max area\n",
    "    maxFaceIndex = findMaxFace(faces)\n",
    "    (x,y,w,h) = faces[maxFaceIndex].copy()\n",
    "    # Cropping Properties\n",
    "    # Modify the x, y, w, h value to include only the central 50% of the total width and 90% of the total height\n",
    "    x = int(x + 0.25 * w)\n",
    "    w = int(w * 0.5)\n",
    "    h = int(h * 0.9)\n",
    "    # Round width and height to the nearst 2^level multiple\n",
    "    # This is so that during the Pyramid reconstruction, the upsampling preserves original dimensions\n",
    "    w = w + int(2**levels)\n",
    "    w = w - (w % int(2**levels))\n",
    "    h = h + int(2**levels)\n",
    "    h = h - (h % int(2**levels))\n",
    "    return (x,y,w,h)\n",
    "\n",
    "def getFace(image, faceTuple):\n",
    "    (x,y,w,h) = faceTuple\n",
    "    # Extract the Cropped Facial Frame. Check the Function above for the Cropping Properties\n",
    "    faceFrame = image[y:y + h, x:x+w, :]\n",
    "    # Blacken out the eyes\n",
    "    faceFrame[int(0.2*h):int(0.55*h), :, :] = 0\n",
    "    return faceFrame\n",
    "    \n",
    "\n",
    "def findMaxFace(faces):\n",
    "    # Find the facila region with max area\n",
    "    # Usually in our case theres only one face. Hence, it gets detected as the max\n",
    "    # But in the scenario with 2 faces, detect the max area\n",
    "    maximum = -1\n",
    "    for i,(x,y,w,h) in enumerate(faces):\n",
    "        if(maximum < w * h):\n",
    "            maximum = w * h\n",
    "            n = i \n",
    "    return n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolate a set of time series data and Filter the interpolated data\n",
    "\n",
    "Takes in the set of time series data as a matrix\n",
    "\n",
    "1) Interpolation Paramters\n",
    "\n",
    "**inputMatrix** => Each row is one time instance. Each column is one feature/corner point i.e. movement of 1-point across time<br/>\n",
    "**inFr** => Input Sampling Frequency i.e. Frame Rate<br/>\n",
    "**outFr** => Output Sampling Frequency (of the pulse oximeter)\n",
    "\n",
    "2) Filter Parameters\n",
    "\n",
    "**lowerCutoff** => Lower Cutoff Frequency in Hz <br/>\n",
    "**higherCutoff** => Upper Cutoff Frequency in Hz <br/>\n",
    "**filterOrder** => Filter Order of the ButterWorth Filter. Due to the use of filtfilt the filter order is actually twice of the specified value <br/>\n",
    "\n",
    "The function interpolates each feature point(column) from the camera FPS rate to the oximeter rate. The interpolated columns are also bandpass filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def interpolationAndFiltering(inputMatrix, inFr, outFr, lowerCutoff=0.75, higherCutoff=5, filterOrder=5):\n",
    "    rows, columns = inputMatrix.shape\n",
    "    # Transfrom the cutoff frequencies from the analog domain to the digital domain\n",
    "    lowerCutoffDigital = lowerCutoff / (0.5 * outFr)\n",
    "    higherCutoffDigital = higherCutoff / (0.5 * outFr)\n",
    "    Fr = outFr/inFr\n",
    "    outputMatrix = np.zeros((int(Fr*rows), columns))\n",
    "    for i in range(columns):\n",
    "        #Interpolate the Data\n",
    "        inputCol = inputMatrix[:,i]\n",
    "        inputCol = cv2.resize(inputCol.reshape([len(inputCol.ravel()),1]), (1, int(Fr * rows)), interpolation = cv2.INTER_CUBIC)\n",
    "        # Filter the data with a Butterworth bandpass filter and a filtfilt operation for a zero-phase response\n",
    "        b, a = signal.butter(filterOrder, [lowerCutoffDigital, higherCutoffDigital], btype='band', analog=False)\n",
    "        inputCol = signal.filtfilt(b, a, inputCol.ravel())\n",
    "        outputMatrix[:,i] = inputCol.ravel()\n",
    "    return outputMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Video Pyramid Decomposition\n",
    "\n",
    "1) **build_gaussian_pyramid** => Takes in an image and forms a Gaussian Pyramid. Levels need to be Specified\n",
    "\n",
    "2) **gaussian_video** => Iterates through the video data(tensor form) and preapers a Gaussian Pyramid for each frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gaussian_pyramid(src,level=3):\n",
    "    s=src.copy()\n",
    "    pyramid=[s]\n",
    "    for i in range(level):\n",
    "        s=cv2.pyrDown(s)\n",
    "        pyramid.append(s)\n",
    "    return pyramid\n",
    "\n",
    "# build gaussian pyramid for video\n",
    "def gaussian_video(video_tensor,levels=3):\n",
    "    for i in range(0,video_tensor.shape[0]):\n",
    "        frame=video_tensor[i]\n",
    "        pyr=build_gaussian_pyramid(frame,level=levels)\n",
    "        gaussian_frame=pyr[-1]\n",
    "        if i==0:\n",
    "            vid_data=np.zeros((video_tensor.shape[0],gaussian_frame.shape[0],gaussian_frame.shape[1],3))\n",
    "        vid_data[i]=gaussian_frame\n",
    "    return vid_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Temporal Ideal Bandpass Filter to Gaussian Video\n",
    "\n",
    "Remove all components outside the range of a human heartbeat\n",
    "\n",
    "**tensor** => The Tensor which containes the Gaussian Pyramid of each Frame<br/>\n",
    "**low** => Lower Cutoff Frequency<br/>\n",
    "**high** => Upper Cutoff Frequency<br/>\n",
    "**fps** => Frame Rate of the Camera<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_ideal_filter(tensor,low,high,fps,axis=0):\n",
    "    fft=fftpack.fft(tensor,axis=axis)\n",
    "    frequencies = fftpack.fftfreq(tensor.shape[0], d=1.0 / fps)\n",
    "    bound_low = (np.abs(frequencies - low)).argmin()\n",
    "    bound_high = (np.abs(frequencies - high)).argmin()\n",
    "    fft[:bound_low] = 0\n",
    "    fft[bound_high:-bound_high] = 0\n",
    "    fft[-bound_low:] = 0\n",
    "    iff=fftpack.ifft(fft, axis=axis)\n",
    "    return np.abs(iff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconstruct Video from Original Video and Gaussian Video\n",
    "\n",
    "Takes the origianl video and adds the amplified changes to it\n",
    "\n",
    "**amp\\_video** => Amplified Video(Pyramid). Tensor Format<br/>\n",
    "**origin\\_video** => Origianl Video. Tendor Format<br/>\n",
    "**levels** => Levels for Image Pyramid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstructVideo(amp_video,origin_video,levels=3):\n",
    "    final_video=np.zeros(origin_video.shape)\n",
    "    for i in range(0,amp_video.shape[0]):\n",
    "        img = amp_video[i]\n",
    "        for x in range(levels):\n",
    "            img=cv2.pyrUp(img)\n",
    "        img=img+origin_video[i]\n",
    "        final_video[i]=img\n",
    "    return final_video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peak Amplification\n",
    "\n",
    "Performs FFT peak and first harmonic amplification\n",
    "\n",
    "1) Takes in a signal and the peak frequency<br/>\n",
    "2) Prepares 2 filters at the max frequency and its first harmonic with the assigned Q factor<br/>\n",
    "3) Filters the wave with each of the 2 filters seperately and then add thier results<br/>\n",
    "4) Output i.e. a peak frequency enchanced version of the signal is returned <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peakAmplification(chosenSignal, outFr, f0, Q=2):\n",
    "    b1, a1 = signal.iirpeak(f0, Q, outFr)\n",
    "    peakFiltered = signal.filtfilt(b1, a1, chosenSignal.ravel())\n",
    "    b2, a2 = signal.iirpeak(f0/2, Q, outFr)\n",
    "    harmonicFiltered = signal.filtfilt(b2, a2, chosenSignal.ravel())\n",
    "    return peakFiltered + harmonicFiltered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the path for the Video File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './Dataset/Data/01-01.mp4' # Replace with the video source file\n",
    "cap = cv2.VideoCapture(path) \n",
    "assert cap.isOpened(), 'Cannot capture source'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters and Constants\n",
    "\n",
    "**inFr** => Input Frequency in Hz<br/>\n",
    "**outFr** => Output Frequency in Hz<br/>\n",
    "**amplification** => Amplification Factor for the Eulerian Video Magnification<br/>\n",
    "**levels** => Levels for the Spatial Decomposition for Video Magnification<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inFr  = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "outFr = 60\n",
    "amplification = 20\n",
    "levels = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROI Extraction\n",
    "\n",
    "Read all the video frames, extract the facial region and save them in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "videoTensor = []\n",
    "while(True):\n",
    "    ret, frame = cap.read()\n",
    "    if ret == False:\n",
    "        break\n",
    "    # Convert to Grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    if counter == 0:\n",
    "        # Extract ROI\n",
    "        faceTuple = haarCascadeFaceDet(gray)\n",
    "    # Save ROI\n",
    "    videoTensor.append(getFace(frame,faceTuple))        \n",
    "    counter += 1\n",
    "cap.release()\n",
    "videoTensor = np.array(videoTensor)\n",
    "print(\"Video Tensor Ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eulerian Video Magnification\n",
    "\n",
    "Color Magnification of the Input Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gau_video = gaussian_video(videoTensor,levels=levels)\n",
    "print(\"Pyramid Fomation Complete. Shape :: \", gau_video.shape)\n",
    "filtered_tensor = temporal_ideal_filter(gau_video,0.8,3,inFr)\n",
    "print(\"Filter Complete\")\n",
    "amplifiedVideo = filtered_tensor * amplification\n",
    "reconstructedTensor = reconstructVideo(amplifiedVideo,videoTensor,levels=levels)\n",
    "print(\"Reconstruction Complete. Shape :: \", reconstructedTensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Averaging over the Width and Height and Interpolate\n",
    "\n",
    "\n",
    "Sum over Width and Breadth to get the Time vs BGR components<br/>\n",
    "Note : Format is B,G,R due to OpenCV<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BGRComponents = np.mean(reconstructedTensor, axis = 1)\n",
    "BGRComponents = np.mean(BGRComponents, axis = 1)\n",
    "BGRComponents = interpolationAndFiltering(BGRComponents, inFr, outFr)\n",
    "\n",
    "for i in range(3):\n",
    "    plt.plot(BGRComponents[:,i])\n",
    "    plt.title('Signal '+str(i))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Component Analysis (Eg: PCA, ICA, etc..)\n",
    "\n",
    "Used to remove the trend from the fluctuations<br/>\n",
    "The fluctuation is the high frequency component (Pulse)<br/>\n",
    "The trend is a very low frequency component<br/>\n",
    "The actual input is a combination of both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Components\n",
    "model = PCA(n_components = 3)\n",
    "principalComponents = model.fit_transform(BGRComponents)\n",
    "\n",
    "for i in range(3):\n",
    "    plt.plot(principalComponents[:,i])\n",
    "    plt.title('Signal '+str(i))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the Best Component\n",
    "\n",
    "1) Take each of the **3** components and analyze them to choose the best signal for the ECG estimation<br/>\n",
    "2) Extract the power spectrum<br/>\n",
    "3) Extract the Frequency component with the max energy (max power value) in itself and its harmonic<br/>\n",
    "4) Find the ration between the above extracted power to the power in the whole power spectrum<br/>\n",
    "5) Choose the signal with the Highest Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyquist = int(len(principalComponents)/2)  \n",
    "x_disp = outFr/2*np.arange(nyquist)/nyquist\n",
    "# Selecte Best Component. Usually the Second one i.e Index = 1\n",
    "powerRatio = []\n",
    "listForDistanceEstimation = []\n",
    "for i in range(3):\n",
    "    fftData = np.fft.fft(principalComponents[:,i])[0:nyquist]\n",
    "    powerSpectrum = np.abs(fftData)**2\n",
    "    maxFreq = np.argmax(powerSpectrum)\n",
    "    powerInMaxFreq = np.sum(powerSpectrum[maxFreq-1:maxFreq+2]) + np.sum(powerSpectrum[2*maxFreq:2*maxFreq+3])\n",
    "    powerRatio.append(powerInMaxFreq/np.sum(powerSpectrum))\n",
    "    listForDistanceEstimation.append((maxFreq)/nyquist*outFr/2)\n",
    "# Choose Signal \n",
    "PCAIndex = np.argmax(np.array(powerRatio))\n",
    "chosenSignal = principalComponents[:,PCAIndex]\n",
    "\n",
    "print(\"Chosen Signal :: \", PCAIndex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amplify Peak and its First Harmonic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosenSignal = peakAmplification(chosenSignal, outFr = outFr, f0 = listForDistanceEstimation[PCAIndex])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peak Det and Display\n",
    "\n",
    "distance = int(outFr*1.6/listForDistanceEstimation[PCAIndex]) # Emperically found realtion via finetuning\n",
    "peaks, _ = signal.find_peaks(chosenSignal, distance=distance)\n",
    "plt.plot(chosenSignal)\n",
    "plt.plot(peaks, chosenSignal[peaks], \"x\")\n",
    "plt.title(\"Avg Heart Beat = \"+str(60*60/(peaks[-1] - peaks[0])*len(peaks)))\n",
    "plt.show()\n",
    "print(\"Number of Peaks :: \", len(peaks))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
