{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulse Detection from Head Movements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all essential libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from scipy import signal, stats\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Head/ROI Dectection\n",
    "\n",
    "1) **haarCascadeFaceDet** :: Takes in an image and outputs the x,y co-ordinates of the upper left corner of the phase with the width and height as well<br/><br/>\n",
    "2) **getFace** :: Takes in an image and a tuple with containg the (x,y,w,h) properties of a bounding box and outputs the facial reigon. The eyes are blacked out here<br/><br/>\n",
    "3) **findMaxFace** :: Takes in a list of faces detected by the harcascade and outputs the (x,y,w,h) tuple corresponding to the face with the maximum area<br/><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haarCascadeFaceDet(image):\n",
    "    # Define the Haar Cascade\n",
    "    face_cascade = cv2.CascadeClassifier('./resources/haarcascade_frontalface_default.xml')\n",
    "    # Use the cascade to extract the face\n",
    "    faces = face_cascade.detectMultiScale(image, 1.3, 5)\n",
    "    # Reain the regoin with the maxmimum face i.e facial reigon with the max area\n",
    "    maxFaceIndex = findMaxFace(faces)\n",
    "    (x,y,w,h) = faces[maxFaceIndex].copy()\n",
    "    # Cropping Properties\n",
    "    # Modify the x, y, w, h value to include only the central 50% of the total width and 90% of the total height\n",
    "    x = int(x + 0.25 * w)\n",
    "    w = int(w * 0.5)\n",
    "    h = int(h * 0.9)\n",
    "    faceFrame = getFace(image, (x,y,w,h))\n",
    "    return faceFrame, (x,y,w,h)\n",
    "\n",
    "def getFace(image, faceTuple):\n",
    "    (x,y,w,h) = faceTuple\n",
    "    # Extract the Cropped Facial Frame. Check the Function above for the Cropping Properties\n",
    "    faceFrame = image[y:y + h, x:x+w]\n",
    "    # Blacken out the eyes\n",
    "    faceFrame[int(0.2*h):int(0.55*h), :] = 0\n",
    "    return faceFrame\n",
    "    \n",
    "\n",
    "def findMaxFace(faces):\n",
    "    # Find the facila region with max area\n",
    "    # Usually in our case theres only one face. Hence, it gets detected as the max\n",
    "    # But in the scenario with 2 faces, detect the max area\n",
    "    maximum = -1\n",
    "    for i,(x,y,w,h) in enumerate(faces):\n",
    "        if(maximum < w * h):\n",
    "            maximum = w * h\n",
    "            n = i \n",
    "    return n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the Extracted Raw Data \n",
    "\n",
    "**rawData** => Each row is one time instance. Each column is one feature/corner point i.e. movement of 1-point across time\n",
    "\n",
    "1) Subtract the features from each time instance with the features from the next time step<br/>\n",
    "2) Find the maximum of the difference along each column i.e. the time instances<br/>\n",
    "3) Find the mode of this distribution<br/>\n",
    "4) Retain all features (columns) whose max difference value is less than or equal to the the mode of the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processRawData(rawData):\n",
    "    # Remove Feature that couldnt be tracked\n",
    "    rawData = rawData[:,:,1]\n",
    "    tempData = np.abs(rawData[1:, :] - rawData[0:-1, :])\n",
    "    # Remove unstable features\n",
    "    maxData  = np.amax(tempData, axis = 0).ravel()\n",
    "    modeData, _ = stats.mode(maxData, axis = None)\n",
    "    processedData = rawData[:, maxData <= modeData]\n",
    "    print(\"Stable Data Shape \", processedData.shape)\n",
    "    return processedData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolate a set of time series data and Filter the interpolated data\n",
    "\n",
    "Takes in the set of time series data as a matrix\n",
    "\n",
    "1) Interpolation Paramters\n",
    "\n",
    "**inputMatrix** => Each row is one time instance. Each column is one feature/corner point i.e. movement of 1-point across time<br/>\n",
    "**inFr** => Input Sampling Frequency i.e. Frame Rate<br/>\n",
    "**outFr** => Output Sampling Frequency (of the pulse oximeter)\n",
    "\n",
    "2) Filter Parameters\n",
    "\n",
    "**lowerCutoff** => Lower Cutoff Frequency in Hz <br/>\n",
    "**higherCutoff** => Upper Cutoff Frequency in Hz <br/>\n",
    "**filterOrder** => Filter Order of the ButterWorth Filter. Due to the use of filtfilt the filter order is actually twice of the specified value <br/>\n",
    "\n",
    "The function interpolates each feature point(column) from the camera FPS rate to the oximeter rate. The interpolated columns are also bandpass filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolationAndFiltering(inputMatrix, inFr, outFr, lowerCutoff=0.75, higherCutoff=4, filterOrder=5):\n",
    "    rows, columns = inputMatrix.shape\n",
    "    # Transfrom the cutoff frequencies from the analog domain to the digital domain\n",
    "    lowerCutoffDigital = lowerCutoff / (0.5 * outFr)\n",
    "    higherCutoffDigital = higherCutoff / (0.5 * outFr)\n",
    "    Fr = outFr/inFr\n",
    "    outputMatrix = np.zeros((int(Fr*rows), columns))\n",
    "    for i in range(columns):\n",
    "        #Interpolate the Data\n",
    "        inputCol = inputMatrix[:,i]\n",
    "        inputCol = cv2.resize(inputCol.reshape([len(inputCol.ravel()),1]), (1, int(Fr * rows)), interpolation = cv2.INTER_CUBIC)\n",
    "        # Filter the data with a Butterworth bandpass filter and a filtfilt operation for a zero-phase response\n",
    "        b, a = signal.butter(filterOrder, [lowerCutoffDigital, higherCutoffDigital], btype='band', analog=False)\n",
    "        inputCol = signal.filtfilt(b, a, inputCol.ravel())\n",
    "        outputMatrix[:,i] = inputCol.ravel()\n",
    "    return outputMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute PCA\n",
    "\n",
    "**filteredData** => Post-processed data(Interpolated and Filtered Data <br/>\n",
    "**n\\_components** => Number of Principle Components needed <br/>\n",
    "**alpha** => Fraction of time instances to be removed for PCA. Values the largest l-2 norm are removed <br/>\n",
    "\n",
    "1) The l-2 norm of the data for each time instance(i.e. for each row) is found<br/>\n",
    "2) The top *aplha* fraction(of time instances) is removed. We call this *tempdata*<br/>\n",
    "3) The *tempData* is zero centered and passed throught the PCA model<br/>\n",
    "4) The learnt model is used to transform the original *filteredData*(all time instances)<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computePCA(filteredData, n_components = 5, alpha = 0.25):\n",
    "    tempData = filteredData.copy()\n",
    "    # Remove top 25% using the L-2 norm\n",
    "    normData = np.linalg.norm(tempData, axis = 1).ravel()\n",
    "    indicies = np.argsort(normData)\n",
    "    topLimit = int( (1 - alpha) * len(indicies))\n",
    "    # Sorted in ascending order. Hence, take time instances lesser than the top 25 %\n",
    "    tempData = tempData[np.sort(indicies[:topLimit]).ravel(), :] \n",
    "    # Fit the PCA model\n",
    "    meanRow = np.mean(tempData, axis = 0)\n",
    "    tempData =  tempData - meanRow\n",
    "    pca = PCA(n_components = n_components)\n",
    "    pca.fit(tempData)\n",
    "    # Apply the PCA model\n",
    "    principalComponents = pca.transform(filteredData)\n",
    "    return principalComponents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peak Amplification\n",
    "\n",
    "Performs FFT peak and first harmonic amplification\n",
    "\n",
    "1) Takes in a signal and the peak frequency<br/>\n",
    "2) Prepares 2 filters at the max frequency and its first harmonic with the assigned Q factor<br/>\n",
    "3) Filters the wave with each of the 2 filters seperately and then add thier results<br/>\n",
    "4) Output i.e. a peak frequency enchanced version of the signal is returned <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peakAmplification(chosenSignal, outFr, f0, Q):\n",
    "    b1, a1 = signal.iirpeak(f0, Q, outFr)\n",
    "    peakFiltered = signal.filtfilt(b1, a1, chosenSignal.ravel())\n",
    "    b2, a2 = signal.iirpeak(2*f0, Q, outFr)\n",
    "    harmonicFiltered = signal.filtfilt(b2, a2, chosenSignal.ravel())\n",
    "    return peakFiltered + harmonicFiltered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the path for the Video File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './Dataset/Data/03-01.mp4' # Replace with the video source file\n",
    "cap = cv2.VideoCapture(path) \n",
    "assert cap.isOpened(), 'Cannot capture source'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters and Constants\n",
    "\n",
    "**inFr** => Input Frequency in Hz<br/>\n",
    "**outFr** => Output Frequency in Hz<br/>\n",
    "**n\\_components** => Number of Components for PCA (or ICA, JADE etc..)<br/>\n",
    "**removeTopComponents** => Remove the top **x%** of the data before PCA (or ICA, JADE etc..)<br/>\n",
    "**Q\\_Factor** => Q factor for the Peak Filters for FFT peak amplfication. To remove unncessary noise in the Spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inFr  = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "outFr = 60\n",
    "n_components = 5\n",
    "removeTopComponents = 25\n",
    "Q_Factor = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters for Corner Detection, Refinement and Lucas Kanade Opticflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params for ShiTomasi corner detection\n",
    "FEATURE_PARAM = dict( maxCorners = 100,\n",
    "                       qualityLevel = 0.01,\n",
    "                       minDistance = 7,\n",
    "                       blockSize = 7 )\n",
    "\n",
    "# Corner Sub pixel\n",
    "CRITERIA = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.001)\n",
    "\n",
    "# Parameters for lucas kanade optical flow\n",
    "LK_PARAMS = dict( winSize  = (100,5),\n",
    "                  maxLevel = 17,\n",
    "                  criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 40, 0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROI Extraction and Feature Tracking\n",
    "\n",
    "1) Read all the video frames and extract the facial region<br/>\n",
    "2) For the first frame, extract the Shi-Tomasi Corner and then refine them<br/>\n",
    "3) For the remaning frames, use the Lucas Kanade Optic Flow to track these points<br/>\n",
    "4) Append all the points from each frame into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "cornerList = []\n",
    "while(True):\n",
    "    ret, frame = cap.read()\n",
    "    if ret == False:\n",
    "        break\n",
    "    newFrame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Find Corner and then Track it\n",
    "    if counter == 0:\n",
    "        faceFrame, faceTuple = haarCascadeFaceDet(newFrame)\n",
    "        # Remember the First Frame\n",
    "        savedFrame = faceFrame\n",
    "        # Extract the  Corners\n",
    "        p0 = cv2.goodFeaturesToTrack(savedFrame, mask = None, **FEATURE_PARAM)\n",
    "        p0 = cv2.cornerSubPix(savedFrame,p0,(15,15),(-1,-1),CRITERIA)\n",
    "        cornerList.append(np.round(p0.reshape(-1,2)))\n",
    "    else :\n",
    "        faceFrame = getFace(newFrame, faceTuple)\n",
    "        # Use the Lucas Kanade algorithm to determine the feature point locations in the new image\n",
    "        p1, st, err = cv2.calcOpticalFlowPyrLK(savedFrame, faceFrame, p0, None, **LK_PARAMS)\n",
    "        cornerList.append(np.round(p1.reshape(-1,2)))\n",
    "    for i in cornerList[counter]:\n",
    "        x,y = i.ravel()\n",
    "        cv2.circle(faceFrame, (x,y), 1, 255, -1)\n",
    "    counter += 1\n",
    "# Release the Video capture and frame\n",
    "cap.release()\n",
    "# Create the Corner feature Matrix\n",
    "cornerList.pop(0)\n",
    "rawData = np.array(cornerList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the Data to Remove Extremities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processedData = processRawData(rawData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolate and Filter the Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredData = interpolationAndFiltering(processedData,inFr,outFr)\n",
    "\n",
    "for i in range(3):\n",
    "    plt.plot(filteredData[:,i])\n",
    "    plt.title('Signal '+str(i))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Component Analysis (Eg: PCA, ICA, etc..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "principalComponents = computePCA(filteredData, n_components = n_components, alpha = removeTopComponents/100)\n",
    "\n",
    "for i in range(5):\n",
    "    plt.plot(principalComponents[:,i])\n",
    "    plt.title('Signal '+str(i))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the Best Component\n",
    "\n",
    "1) Take each of the **n** components and analyze them to choose the best signal for the ECG estimation<br/>\n",
    "2) Extract the power spectrum<br/>\n",
    "3) Extract the Frequency component with the max energy (max power value) in itself and its harmonic<br/>\n",
    "4) Find the ration between the above extracted power to the power in the whole power spectrum<br/>\n",
    "5) Choose the signal with the Highest Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nyquist = int(len(principalComponents)/2)\n",
    "powerRatio = []\n",
    "listForDistanceEstimation = []\n",
    "for i in range(principalComponents.shape[1]):\n",
    "    fftData = np.fft.fft(principalComponents[:,i])[1:nyquist]\n",
    "    powerSpectrum = np.abs(fftData)**2\n",
    "    maxFreq = np.argmax(powerSpectrum)\n",
    "#     print(i, \"     \", (maxFreq+1)/nyquist*outFr/2)\n",
    "    powerInMaxFreq = np.sum(powerSpectrum[maxFreq-1:maxFreq+2]) #+ np.sum(powerSpectrum[2*maxFreq:2*maxFreq+3])\n",
    "    powerRatio.append(powerInMaxFreq/np.sum(powerSpectrum))\n",
    "    listForDistanceEstimation.append((maxFreq+1)/nyquist*outFr/2)\n",
    "PCAIndex = np.argmax(np.array(powerRatio))\n",
    "chosenSignal = principalComponents[:,PCAIndex]\n",
    "\n",
    "print(\"Chosen Signal :: \", PCAIndex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amplify Peak and its First Harmonic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosenSignal = peakAmplification(chosenSignal, outFr = outFr, f0 = listForDistanceEstimation[PCAIndex], Q = Q_Factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_disp = outFr/2*np.arange(nyquist)/nyquist\n",
    "x_disp = x_disp[1:]\n",
    "distance = int(0.7*outFr/listForDistanceEstimation[PCAIndex]) # Emperically found realtion\n",
    "peaks, _ = signal.find_peaks(chosenSignal, distance=distance,  height = np.mean(chosenSignal))\n",
    "print(\"Number of Peaks :: \", len(peaks))\n",
    "plt.plot(chosenSignal)\n",
    "plt.plot(peaks, chosenSignal[peaks], \"x\")\n",
    "plt.title(\"Avg Heart Beat = \"+str(listForDistanceEstimation[PCAIndex]*60))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
